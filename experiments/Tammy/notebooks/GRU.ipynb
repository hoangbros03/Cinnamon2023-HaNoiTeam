{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cell of GRU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, bias: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize gated recurrent unit cell\n",
    "        Parameters\n",
    "        --------\n",
    "          input_size: int\n",
    "            The number of expected features in the input x\n",
    "          hidden_size: int\n",
    "            The number of features in the hidden state h\n",
    "          bias: bool\n",
    "            Optional, if False,the layer doesn't use bias weights b_ih and b_hh\n",
    "            Default: True\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # reset gate (r)\n",
    "        self.reset_i2r = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.reset_h2r = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # update gate (z)\n",
    "        self.update_i2z = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.update_h2z = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # almost output (n)\n",
    "        self.output_i2n = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.output_h2n = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def reset_gate(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        r: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.reset_i2r(x)\n",
    "        hs_pre = self.reset_h2r(h)\n",
    "        acti = nn.Sigmoid()\n",
    "        r = acti(x_t + hs_pre)\n",
    "        return r\n",
    "\n",
    "    def update_gate(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        z: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.update_i2z(x)\n",
    "        hs_pre = self.update_h2z(h)\n",
    "        acti = nn.Sigmoid()\n",
    "        z = acti(x_t + hs_pre)\n",
    "        return z\n",
    "\n",
    "    def almost_output(\n",
    "        self, x: torch.Tensor, h: torch.Tensor, r: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        r: size (batch_size, hidden_size)\n",
    "        n: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.output_i2n(x)\n",
    "        hs_pre = self.output_h2n(h)\n",
    "        acti = nn.Tanh()\n",
    "        n = acti(x_t + hs_pre)\n",
    "        return n\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the GRU cell\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, input_size).\n",
    "            hs_pre: torch.Tensor\n",
    "                Previous hidden state tensor of shape (batch_size, hidden_size)\n",
    "                 Default is None, the initial hidden state is set to zeros.\n",
    "        Returns\n",
    "        -------\n",
    "            hs: torch.Tensor\n",
    "                Output hidden state tensor of shape (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_size)\n",
    "        r = self.reset_gate(x, h)\n",
    "        z = self.update_gate(x, h)\n",
    "        n = self.almost_output(x, h, r)\n",
    "        hs = (1 - z) * n + z * h\n",
    "        return hs\n",
    "\n",
    "    def init_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the RNN cell\n",
    "        followed by Xavier normalization\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            if \"bias\" in name:\n",
    "                param = param.view(1, param.size(0))\n",
    "                torch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a multi-layer GRU model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize gated recurrent unit\n",
    "        Parameters\n",
    "        --------\n",
    "          input_size: int\n",
    "            The number of expected features in the input x\n",
    "          hidden_size: int\n",
    "            The number of features in the hidden state h\n",
    "          num_layers: int\n",
    "            The number of layers\n",
    "          output_size: int\n",
    "            The number of output features\n",
    "          bias: bool\n",
    "            Optional, if False, then the layer does not use bias weights\n",
    "            b_ih and b_hh. Default: True\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.bias = bias\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.init_cell_list()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hs_pre: torch.Tensor = None) ->tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the GRU model.\n",
    "        Parameters\n",
    "        --------\n",
    "          input: torch.Tensor\n",
    "            The input tensor of shape (batch_size, sequence_length, input_size)\n",
    "          hs_pre: torch.Tensor\n",
    "            Previous hidden state tensor of shape (batch_size, hidden_size)\n",
    "            Default is None, the initial hidden state is set to zeros.\n",
    "        Returns: tuple[torch.Tensor, torch.Tensor]\n",
    "            out: torch.Tensor\n",
    "                Output tensor of shape (batch_size, output_size)\n",
    "            hs_final: torch.Tensor\n",
    "                Final hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        output = []\n",
    "        hidden_layers = list(hs_pre)\n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden = self.gru_cell_list[layer].forward(\n",
    "                        input[:, t, :], hidden_layers[layer]\n",
    "                    )\n",
    "                else:\n",
    "                    hidden = self.gru_cell_list[layer].forward(\n",
    "                        hidden_layers[layer - 1], hidden_layers[layer]\n",
    "                    )\n",
    "                hidden_layers[layer] = hidden\n",
    "            output.append(hidden)\n",
    "        out = output[-1].squeeze()\n",
    "        hs_final = torch.stack(hidden_layers, dim=0)\n",
    "        return out, hs_final \n",
    "\n",
    "    def init_cell_list(self):\n",
    "        \"\"\"\n",
    "        Initializes the GRU cell list based on the number of layers.\n",
    "        \"\"\"\n",
    "        self.gru_cell_list = nn.ModuleList()\n",
    "        self.gru_cell_list.append(GRUCell(self.input_size, self.hidden_size, self.bias))\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.gru_cell_list.append(\n",
    "                GRUCell(self.hidden_size, self.hidden_size, self.bias)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28  # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1  # number of hidden layers\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "seq_len = 28\n",
    "test = GRU(28, 100, 10, 1, True)\n",
    "x = torch.rand( seq_len, batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 100]) tensor([[-0.2668, -0.5939, -0.0206,  ..., -0.5574, -0.2525,  0.4538],\n",
      "        [-0.1837, -0.5600,  0.0968,  ..., -0.5274, -0.4458,  0.3581],\n",
      "        [ 0.0568, -0.4267,  0.1676,  ..., -0.5104, -0.4412,  0.5746],\n",
      "        ...,\n",
      "        [-0.2874, -0.3609,  0.2265,  ..., -0.4313, -0.4350,  0.4398],\n",
      "        [ 0.2242, -0.4099,  0.2173,  ..., -0.5283, -0.4649,  0.4902],\n",
      "        [-0.1249, -0.3885,  0.0440,  ..., -0.1153, -0.2531,  0.6546]],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[0].size(), test.forward(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 100]) tensor([[[-0.2668, -0.5939, -0.0206,  ..., -0.5574, -0.2525,  0.4538],\n",
      "         [-0.1837, -0.5600,  0.0968,  ..., -0.5274, -0.4458,  0.3581],\n",
      "         [ 0.0568, -0.4267,  0.1676,  ..., -0.5104, -0.4412,  0.5746],\n",
      "         ...,\n",
      "         [-0.2874, -0.3609,  0.2265,  ..., -0.4313, -0.4350,  0.4398],\n",
      "         [ 0.2242, -0.4099,  0.2173,  ..., -0.5283, -0.4649,  0.4902],\n",
      "         [-0.1249, -0.3885,  0.0440,  ..., -0.1153, -0.2531,  0.6546]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[1].size(), test.forward(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
