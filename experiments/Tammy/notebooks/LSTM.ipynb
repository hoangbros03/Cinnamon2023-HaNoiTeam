{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cell of LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize long short term memory cell\n",
    "        Parameters\n",
    "        --------\n",
    "          input_size: int\n",
    "            The number of expected features in the input x\n",
    "          hidden_size: int\n",
    "            The number of features in the hidden state h\n",
    "          bias: bool\n",
    "            Optional, if False, the layer doesn't use bias weights b_ih and b_hh\n",
    "            Default: True\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # input gate (i)\n",
    "        self.input_x2i = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.input_h2i = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # forgot gate (f)\n",
    "        self.forgot_x2f = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.forgot_h2f = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # cell vector (c)\n",
    "        self.cell_x2c = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.cell_h2c = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # almost output (o)\n",
    "        self.output_x2o = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.output_h20 = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def input_gate(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        i: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.input_x2i(x)\n",
    "        hs_pre = self.input_h2i(h)\n",
    "        acti = nn.Sigmoid()\n",
    "        i = acti(x_t + hs_pre)\n",
    "        return i\n",
    "\n",
    "    def forgot_gate(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        f: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.forgot_x2f(x)\n",
    "        hs_pre = self.forgot_h2f(h)\n",
    "        acti = nn.Sigmoid()\n",
    "        f = acti(x_t + hs_pre)\n",
    "        return f\n",
    "\n",
    "    def cell_vector(\n",
    "        self,\n",
    "        i: torch.Tensor,\n",
    "        f: torch.Tensor,\n",
    "        x: torch.Tensor,\n",
    "        h: torch.Tensor,\n",
    "        c_pre: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        c: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.cell_x2c(x)\n",
    "        hs_pre = self.cell_h2c(h)\n",
    "        acti = nn.Tanh()\n",
    "        c = f * c_pre + i * acti(x_t + hs_pre)\n",
    "        return c\n",
    "\n",
    "    def almost_output(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: size (batch_size, input_size)\n",
    "        h: size (batch_size, hidden_size)\n",
    "        out: size (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        x_t = self.output_x2o(x)\n",
    "        hs_pre = self.output_h20(h)\n",
    "        acti = nn.Sigmoid()\n",
    "        out = acti(x_t + hs_pre)\n",
    "        return out\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, h_n_c: tuple[torch.Tensor, torch.Tensor] = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the LSTM cell\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, input_size).\n",
    "            h_n_c: tuple[torch.Tensor, torch.Tensor]\n",
    "                Previous hidden state tensor of shape\n",
    "                ((batch_size, hidden_size), (batch_size, hidden_size))\n",
    "                Default is None, the initial hidden state is set to zeros.\n",
    "        Returns\n",
    "        -------\n",
    "            hs: tuple[torch.Tensor, torch.Tensor]\n",
    "                Output hidden state tensor of shape\n",
    "                ((batch_size, hidden_size), (batch_size, hidden_size)).\n",
    "        \"\"\"\n",
    "        if h_n_c is None:\n",
    "            h_n_c = torch.zeros(x.size(0), self.hidden_size)\n",
    "            h_n_c = (h_n_c, h_n_c)\n",
    "        (hs_pre, c_pre) = h_n_c\n",
    "        i = self.input_gate(x, hs_pre)\n",
    "        f = self.forgot_gate(x, hs_pre)\n",
    "        c = self.cell_vector(i, f, x, hs_pre, c_pre)\n",
    "        o = self.almost_output(x, hs_pre)\n",
    "        acti = nn.Tanh()\n",
    "        hs = o * acti(c)\n",
    "        return (hs, c)\n",
    "\n",
    "    def init_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the LSTM cell\n",
    "        followed by Xavier normalization\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            if \"bias\" in name:\n",
    "                param = param.view(1, param.size(0))\n",
    "                torch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a multi-layer LSTM model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize long short term memory\n",
    "        Parameters\n",
    "        --------\n",
    "          input_size: int\n",
    "            The number of expected features in the input x\n",
    "          hidden_size: int\n",
    "            The number of features in the hidden state h\n",
    "          bias: bool\n",
    "            Optional, if False, the layer doesn't use bias weights b_ih and b_hh.\n",
    "            Default: True\n",
    "        Returns\n",
    "        -------\n",
    "          None\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.init_cell_list()\n",
    "\n",
    "    def forward(\n",
    "        self, input: torch.Tensor, hs_pre: tuple[torch.Tensor, torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        Parameters\n",
    "        --------\n",
    "          input: torch.Tensor\n",
    "            The input tensor of shape (batch_size, sequence_length, input_size)\n",
    "          hs_pre: tuple[torch.Tensor, torch.Tensor]\n",
    "            Previous hidden state tensor of shape\n",
    "            ((batch_size, hidden_size), (batch_size, hidden_size))\n",
    "            Default is None, the initial hidden state is set to zeros.\n",
    "        Returns \n",
    "        --------\n",
    "          output: torch.Tensor\n",
    "            Output hidden state tensor of shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        output = []\n",
    "        hidden_layers = list(hs_pre) + list(hs_pre)\n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden = self.lstm_cell_list[layer].forward(\n",
    "                        input[:, t, :],\n",
    "                        (hidden_layers[layer][0], hidden_layers[layer][1]),\n",
    "                    )\n",
    "                else:\n",
    "                    hidden = self.lstm_cell_list[layer].forward(\n",
    "                        hidden_layers[layer - 1][0],\n",
    "                        (hidden_layers[layer][0], hidden_layers[layer][1]),\n",
    "                    )\n",
    "                hidden_layers[layer] = hidden\n",
    "            output.append(hidden[0])\n",
    "        out = output[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def init_cell_list(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the LSTM cell list based on the number of layers.\n",
    "        \"\"\"\n",
    "        self.lstm_cell_list = nn.ModuleList()\n",
    "        self.lstm_cell_list.append(\n",
    "            LSTMCell(self.input_size, self.hidden_size, self.bias)\n",
    "        )\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.lstm_cell_list.append(\n",
    "                LSTMCell(self.hidden_size, self.hidden_size, self.bias)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28  # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1  # number of hidden layers\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "seq_len = 28\n",
    "test = LSTM(28, 100, 10, 1, True)\n",
    "x = torch.rand(seq_len, batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 10]) tensor([[-0.0553, -0.1602, -0.0626,  0.1215, -0.0583, -0.0262, -0.0771,  0.0806,\n",
      "          0.0807, -0.1749],\n",
      "        [-0.0426, -0.1778, -0.0602,  0.1115, -0.0759, -0.0095, -0.0238,  0.0848,\n",
      "         -0.0086, -0.1316],\n",
      "        [-0.0715, -0.1779, -0.0900,  0.1281, -0.0933, -0.0164, -0.0571,  0.0880,\n",
      "         -0.0069, -0.1538],\n",
      "        [-0.0678, -0.2332, -0.1064,  0.0642, -0.0592, -0.0532, -0.0900,  0.0213,\n",
      "          0.0957, -0.1392],\n",
      "        [-0.0528, -0.1844, -0.0919,  0.0895, -0.0864, -0.0220, -0.0283,  0.0564,\n",
      "          0.0245, -0.1547],\n",
      "        [-0.0653, -0.1616, -0.0835,  0.1054, -0.0535, -0.0735, -0.0390,  0.0123,\n",
      "         -0.0159, -0.0699],\n",
      "        [ 0.0066, -0.1381, -0.0798,  0.0270, -0.0076, -0.0236, -0.1256,  0.0473,\n",
      "          0.0929, -0.1238],\n",
      "        [ 0.0110, -0.2156, -0.0829,  0.0544, -0.0336, -0.0467, -0.0835,  0.0675,\n",
      "          0.0370, -0.0876],\n",
      "        [-0.0459, -0.2389, -0.0461,  0.0885, -0.1012, -0.0276, -0.0891, -0.0149,\n",
      "          0.0454, -0.1350],\n",
      "        [-0.0971, -0.1708, -0.0723,  0.0546, -0.0711, -0.0243, -0.0513,  0.0262,\n",
      "          0.0636, -0.1212],\n",
      "        [-0.0410, -0.1539, -0.0943,  0.1384, -0.0543, -0.0430, -0.0465,  0.0902,\n",
      "          0.0626, -0.1351],\n",
      "        [-0.0655, -0.1869, -0.0230,  0.0778, -0.0412, -0.0220, -0.1169,  0.0486,\n",
      "          0.0728, -0.1059],\n",
      "        [-0.1198, -0.1752, -0.0777,  0.1406, -0.0779, -0.0242, -0.0324,  0.0264,\n",
      "          0.0310, -0.1169],\n",
      "        [-0.0736, -0.1874, -0.0830,  0.0995, -0.0719, -0.0076, -0.1104,  0.0116,\n",
      "          0.0926, -0.1761],\n",
      "        [-0.0577, -0.1358, -0.0482,  0.0857, -0.0561, -0.0148, -0.0573,  0.0442,\n",
      "          0.0330, -0.1370],\n",
      "        [-0.0348, -0.1967, -0.0610,  0.0936, -0.0753, -0.0582, -0.0816,  0.0640,\n",
      "          0.0357, -0.1283],\n",
      "        [-0.0846, -0.1947, -0.0576,  0.1158, -0.1010, -0.0497, -0.0704,  0.0512,\n",
      "          0.0811, -0.1569],\n",
      "        [-0.0673, -0.1922, -0.0836,  0.0745, -0.0778, -0.0307, -0.0981,  0.0376,\n",
      "          0.0217, -0.1252],\n",
      "        [-0.1120, -0.1968, -0.0726,  0.0849, -0.0867, -0.0410, -0.0464, -0.0082,\n",
      "          0.1004, -0.0948],\n",
      "        [-0.0247, -0.1980, -0.0885,  0.0640, -0.0529, -0.0456, -0.0149,  0.0522,\n",
      "          0.0465, -0.1093],\n",
      "        [-0.0078, -0.1753, -0.0790,  0.1190, -0.0410, -0.0306, -0.0890,  0.0340,\n",
      "          0.0598, -0.1470],\n",
      "        [-0.0008, -0.1619, -0.0924,  0.0865, -0.0570, -0.0482, -0.1149,  0.0388,\n",
      "          0.0588, -0.1318],\n",
      "        [-0.0448, -0.1820, -0.0554,  0.0912, -0.0548, -0.0380, -0.0687,  0.0563,\n",
      "          0.0943, -0.1981],\n",
      "        [-0.0963, -0.2210, -0.0860,  0.1245, -0.1130, -0.0561, -0.0579,  0.0093,\n",
      "          0.0890, -0.1513],\n",
      "        [-0.0753, -0.2048, -0.0874,  0.1417, -0.0625, -0.0030, -0.0904,  0.0297,\n",
      "         -0.0008, -0.1673],\n",
      "        [-0.1209, -0.1569, -0.0929,  0.1288, -0.0457, -0.0651, -0.0258,  0.0182,\n",
      "          0.1014, -0.1575],\n",
      "        [-0.0151, -0.2062, -0.1008,  0.1090, -0.0913, -0.0519, -0.0286,  0.0779,\n",
      "         -0.0081, -0.1214],\n",
      "        [-0.0666, -0.2062, -0.0898,  0.0875, -0.0828, -0.0721, -0.0515,  0.0183,\n",
      "          0.0307, -0.1121]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x).size(), test.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
