{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cell of RNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, bias: bool, activation: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize recurrent neural network\n",
    "        Parameters\n",
    "        --------\n",
    "            input_size: int\n",
    "                Number of feature in the input x\n",
    "            output_size: int\n",
    "                Number of feature in the output y\n",
    "            hidden_size: int\n",
    "                Number of feature in the hidden state h\n",
    "            bias: bool\n",
    "                Whether to include a bias term in the linear transformations\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state, there are 2\n",
    "                options: tanh and relu\n",
    "        Returns\n",
    "        -------\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        if activation not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "        self.activation = activation\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hs_pre: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the RNN cell\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, input_size).\n",
    "            hs_pre: torch.Tensor\n",
    "                Previous hidden state tensor of shape (batch_size, hidden_size)\n",
    "                 Default is None, the initial hidden state is set to zeros.\n",
    "        Returns\n",
    "        -------\n",
    "            hs: torch.Tensor\n",
    "                Output hidden state tensor of shape (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(input.size(0), self.hidden_size)\n",
    "        hs = self.x2h(input) + self.h2h(hs_pre)\n",
    "        if self.activation == \"tanh\":\n",
    "            hs = torch.tanh(hs)\n",
    "        else:\n",
    "            hs = torch.relu(hs)\n",
    "        return hs\n",
    "\n",
    "    def init_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the RNN cell\n",
    "        followed by Xavier normalization\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            if \"bias\" in name:\n",
    "                param = param.view(1, param.size(0))\n",
    "                torch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement RNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int,\n",
    "        bias: bool,\n",
    "        activation=\"str\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Recurrent Neural Network (RNN) model.\n",
    "        Parameters\n",
    "        --------\n",
    "            input_size: int\n",
    "                Number of features in the input x\n",
    "            hidden_size: int\n",
    "                Number of features in the hidden state h\n",
    "            output_size: int\n",
    "                Number of features in the output y\n",
    "            num_layers: int\n",
    "                Number of RNN cell layers\n",
    "            bias: bool\n",
    "                Whether to include a bias term in the linear transformations\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state,\n",
    "                there are 2 options: tanh and relu\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        if activation not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias = self.bias)\n",
    "        self.init_layer(activation)\n",
    "\n",
    "    def forward(self, input, hs_pre=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the RNN model\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "            hs_pre: torch.Tensor\n",
    "                Previous hidden state tensor of shape\n",
    "                (num_layers, batch_size, hidden_size).\n",
    "                Default is None, the initial hidden state is set to zeros\n",
    "\n",
    "        Returns: tuple[torch.Tensor, torch.Tensor]\n",
    "            out: torch.Tensor\n",
    "                Output tensor of shape (batch_size, output_size)\n",
    "            hs_final: torch.Tensor\n",
    "                Final hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        output = []\n",
    "        hidden_layers = list(hs_pre)\n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden = self.rnn_cell_list[layer].forward(\n",
    "                        input[:, t, :], hidden_layers[layer]\n",
    "                    )\n",
    "                else:\n",
    "                    hidden = self.rnn_cell_list[layer](\n",
    "                        hidden_layers[layer - 1], hidden_layers[layer]\n",
    "                    )\n",
    "                hidden_layers[layer] = hidden\n",
    "            output.append(hidden)\n",
    "        out = output[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        hs_final = torch.stack(hidden_layers, dim=0)\n",
    "        return out, hs_final\n",
    "\n",
    "    def init_layer(self, activation: str):\n",
    "        \"\"\"\n",
    "        Initialize the RNN cell list.\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state,\n",
    "                there are 2 options: tanh and relu\n",
    "\n",
    "        \"\"\"\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "        if activation == \"tanh\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(self.input_size, self.hidden_size, self.bias, \"tanh\")\n",
    "            )\n",
    "            for _ in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(self.hidden_size, self.hidden_size, self.bias, \"tanh\")\n",
    "                )\n",
    "        elif activation == \"relu\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(self.input_size, self.hidden_size, self.bias, \"relu\")\n",
    "            )\n",
    "            for _ in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(self.hidden_size, self.hidden_size, self.bias, \"relu\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28  # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1  # number of hidden layers\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "seq_len = 28\n",
    "test = RNN(28, 100, 10, 1, True, \"tanh\")\n",
    "x = torch.rand( seq_len, batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 10]) tensor([[ 0.0888,  0.3144,  0.3136, -0.0854, -0.0362,  0.0709,  0.8744,  0.5221,\n",
      "          0.0130, -0.0313],\n",
      "        [ 0.0687,  0.1406,  0.0350, -0.0355, -0.3912, -0.2070,  0.6919,  0.5058,\n",
      "          0.0596, -0.2201],\n",
      "        [ 0.0422,  0.2142,  0.1666, -0.2248, -0.0453, -0.0993,  0.2942,  0.2005,\n",
      "         -0.1305, -0.1414],\n",
      "        [-0.0882, -0.0853,  0.4190, -0.2123, -0.0616, -0.1202,  0.4652,  0.2199,\n",
      "         -0.2855, -0.0063],\n",
      "        [-0.1265,  0.2059,  0.3833, -0.2442, -0.3663, -0.2000,  0.6116,  0.5147,\n",
      "         -0.0566, -0.0489],\n",
      "        [-0.1466,  0.1200,  0.0643, -0.3003,  0.1424, -0.0600,  0.7734,  0.5787,\n",
      "         -0.0382, -0.0642],\n",
      "        [-0.2135,  0.0499,  0.1985, -0.3844, -0.1370, -0.0377,  0.7577,  0.4427,\n",
      "         -0.0693, -0.2159],\n",
      "        [-0.2944, -0.0195,  0.2573, -0.2044, -0.1323,  0.0147,  0.7252,  0.4054,\n",
      "         -0.3397, -0.1801],\n",
      "        [-0.1267,  0.1813,  0.3451, -0.0504, -0.3300, -0.0812,  0.6150,  0.6598,\n",
      "          0.2830, -0.1583],\n",
      "        [-0.1989,  0.1061,  0.0394, -0.4785, -0.2637, -0.0338,  1.0668,  0.5917,\n",
      "          0.0117, -0.1720],\n",
      "        [ 0.0028,  0.3333,  0.2351, -0.1918, -0.4058,  0.0102,  0.7874,  0.2522,\n",
      "          0.0040, -0.0080],\n",
      "        [-0.1001,  0.2149,  0.2268, -0.0309, -0.0660, -0.1241,  0.4291,  0.5689,\n",
      "         -0.0352,  0.0122],\n",
      "        [-0.0713,  0.0773,  0.1668, -0.3265, -0.1619, -0.0545,  0.5881,  0.5455,\n",
      "          0.0038, -0.1296],\n",
      "        [-0.1436, -0.0139,  0.1716, -0.5271, -0.3266, -0.2426,  0.5678,  0.5732,\n",
      "         -0.0852,  0.0056],\n",
      "        [-0.3291,  0.0772,  0.1370, -0.3830, -0.1717,  0.0519,  0.5425,  0.4230,\n",
      "          0.0019, -0.0477],\n",
      "        [ 0.0163,  0.4618,  0.4186, -0.3489, -0.1761, -0.0804,  0.3827,  0.4032,\n",
      "          0.1980, -0.0424],\n",
      "        [ 0.1634,  0.2425,  0.0833, -0.3074, -0.0641, -0.1065,  0.8054,  0.6345,\n",
      "          0.0788, -0.1155],\n",
      "        [ 0.1914,  0.2613,  0.1257, -0.4346,  0.0729, -0.1952,  0.7774,  0.4084,\n",
      "          0.0455, -0.1862],\n",
      "        [ 0.0774, -0.0271,  0.3371, -0.1897, -0.2686,  0.2247,  0.6747,  0.2044,\n",
      "         -0.1970,  0.0788],\n",
      "        [ 0.0184,  0.2236,  0.4008, -0.3751, -0.2892, -0.1723,  0.5655,  0.6374,\n",
      "          0.2079, -0.1938],\n",
      "        [ 0.0025,  0.2351,  0.1593,  0.0150,  0.1700, -0.2661,  0.4529,  0.2673,\n",
      "          0.0146, -0.0136],\n",
      "        [-0.1870,  0.1433,  0.1806, -0.2768,  0.0352,  0.0282,  0.4378,  0.5408,\n",
      "         -0.1955, -0.0429],\n",
      "        [ 0.0781, -0.0075,  0.3739, -0.2762, -0.0453, -0.2110,  0.7809,  0.4983,\n",
      "          0.2209, -0.2466],\n",
      "        [-0.2655, -0.0733,  0.1875, -0.3391, -0.1272, -0.0941,  0.6867,  0.3555,\n",
      "          0.0926, -0.0921],\n",
      "        [-0.1480,  0.2065,  0.1364, -0.2432, -0.2793, -0.1869,  0.8008,  0.5600,\n",
      "         -0.1843, -0.2039],\n",
      "        [-0.1655,  0.2456,  0.0943, -0.3264, -0.2587, -0.0646,  0.6898,  0.5379,\n",
      "         -0.1436, -0.0567],\n",
      "        [-0.1291,  0.2109,  0.0560, -0.0759, -0.0850, -0.1569,  0.4389,  0.4705,\n",
      "         -0.0218, -0.1974],\n",
      "        [-0.0781,  0.1889,  0.2850, -0.2551, -0.1686,  0.0659,  0.7001,  0.6468,\n",
      "         -0.0674, -0.0485]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[0].size(), test.forward(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 100]) tensor([[[ 0.0841, -0.4109, -0.1288,  ..., -0.3316, -0.0921, -0.1590],\n",
      "         [ 0.3775, -0.0048, -0.4009,  ...,  0.2302,  0.5344, -0.6408],\n",
      "         [ 0.0831, -0.6643, -0.4147,  ..., -0.1601,  0.0439, -0.4168],\n",
      "         ...,\n",
      "         [ 0.3204, -0.1186, -0.0134,  ..., -0.4212,  0.6957, -0.5847],\n",
      "         [ 0.3830, -0.7463, -0.1409,  ..., -0.0421,  0.3670, -0.8187],\n",
      "         [ 0.1241, -0.5083,  0.0676,  ...,  0.2457,  0.6423, -0.6868]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[1].size(), test.forward(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
