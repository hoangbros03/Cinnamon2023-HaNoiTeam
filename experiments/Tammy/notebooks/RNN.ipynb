{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cell of RNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, bias: bool, activation: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize recurrent neural network\n",
    "        Parameters\n",
    "        --------\n",
    "            input_size: int\n",
    "                Number of feature in the input x\n",
    "            output_size: int\n",
    "                Number of feature in the output y\n",
    "            hidden_size: int\n",
    "                Number of feature in the hidden state h\n",
    "            bias: bool\n",
    "                Whether to include a bias term in the linear transformations\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state, there are 2\n",
    "                options: tanh and relu\n",
    "        Returns\n",
    "        -------\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        if activation not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "        self.activation = activation\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hs_pre: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the RNN cell\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, input_size).\n",
    "            hs_pre: torch.Tensor\n",
    "                Previous hidden state tensor of shape (batch_size, hidden_size)\n",
    "                 Default is None, the initial hidden state is set to zeros.\n",
    "        Returns\n",
    "        -------\n",
    "            hs: torch.Tensor\n",
    "                Output hidden state tensor of shape (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(input.size(0), self.hidden_size)\n",
    "        hs = self.x2h(input) + self.h2h(hs_pre)\n",
    "        if self.activation == \"tanh\":\n",
    "            hs = torch.tanh(hs)\n",
    "        else:\n",
    "            hs = torch.relu(hs)\n",
    "        return hs\n",
    "\n",
    "    def init_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the RNN cell\n",
    "        followed by Xavier normalization\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            if \"bias\" in name:\n",
    "                param = param.view(1, param.size(0))\n",
    "                torch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement RNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int,\n",
    "        bias: bool,\n",
    "        activation=\"str\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Recurrent Neural Network (RNN) model.\n",
    "        Parameters\n",
    "        --------\n",
    "            input_size: int\n",
    "                Number of features in the input x\n",
    "            hidden_size: int\n",
    "                Number of features in the hidden state h\n",
    "            output_size: int\n",
    "                Number of features in the output y\n",
    "            num_layers: int\n",
    "                Number of RNN cell layers\n",
    "            bias: bool\n",
    "                Whether to include a bias term in the linear transformations\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state,\n",
    "                there are 2 options: tanh and relu\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        if activation not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias = self.bias)\n",
    "        self.init_layer(activation)\n",
    "\n",
    "    def forward(self, input, hs_pre=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the forward propagation of the RNN model\n",
    "        Parameters\n",
    "        --------\n",
    "            input: torch.Tensor\n",
    "                Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "            hs_pre: torch.Tensor\n",
    "                Previous hidden state tensor of shape\n",
    "                (num_layers, batch_size, hidden_size).\n",
    "                Default is None, the initial hidden state is set to zeros\n",
    "\n",
    "        Returns: tuple[torch.Tensor, torch.Tensor]\n",
    "            out: torch.Tensor\n",
    "                Output tensor of shape (batch_size, output_size)\n",
    "            hs_final: torch.Tensor\n",
    "                Final hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if hs_pre is None:\n",
    "            hs_pre = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        output = []\n",
    "        hidden_layers = list(hs_pre)\n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden = self.rnn_cell_list[layer].forward(\n",
    "                        input[:, t, :], hidden_layers[layer]\n",
    "                    )\n",
    "                else:\n",
    "                    hidden = self.rnn_cell_list[layer].forward(\n",
    "                        hidden_layers[layer - 1], hidden_layers[layer]\n",
    "                    )\n",
    "                hidden_layers[layer] = hidden\n",
    "            output.append(hidden)\n",
    "        out = output[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        hs_final = torch.stack(hidden_layers, dim=0)\n",
    "        return out, hs_final\n",
    "\n",
    "    def init_layer(self, activation: str):\n",
    "        \"\"\"\n",
    "        Initialize the RNN cell list.\n",
    "        Parameters\n",
    "        --------\n",
    "            None\n",
    "        Returns\n",
    "        -------\n",
    "            activation: str\n",
    "                Activation function to apply to the hidden state,\n",
    "                there are 2 options: tanh and relu\n",
    "\n",
    "        \"\"\"\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "        if activation == \"tanh\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(self.input_size, self.hidden_size, self.bias, \"tanh\")\n",
    "            )\n",
    "            for _ in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(self.hidden_size, self.hidden_size, self.bias, \"tanh\")\n",
    "                )\n",
    "        elif activation == \"relu\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(self.input_size, self.hidden_size, self.bias, \"relu\")\n",
    "            )\n",
    "            for _ in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(self.hidden_size, self.hidden_size, self.bias, \"relu\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28  # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1  # number of hidden layers\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "seq_len = 28\n",
    "test = RNN(28, 100, 10, 1, True, \"tanh\")\n",
    "x = torch.rand( seq_len, batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 10]) tensor([[ 0.3240,  0.1318, -0.3015,  0.2144, -0.0220, -0.0358,  0.1316, -0.1483,\n",
      "         -0.2284,  0.4579],\n",
      "        [ 0.0450,  0.3146, -0.0773,  0.2080, -0.0566, -0.3204, -0.1115,  0.1054,\n",
      "         -0.0486,  0.6179],\n",
      "        [ 0.1036,  0.1987, -0.4135,  0.1749, -0.1761,  0.0322, -0.1121, -0.1917,\n",
      "          0.1592,  0.5641],\n",
      "        [ 0.2084,  0.2986, -0.5458,  0.1055,  0.1568, -0.2350, -0.0763, -0.0389,\n",
      "         -0.1862,  0.2424],\n",
      "        [ 0.2791,  0.0548, -0.1757,  0.2230,  0.0735, -0.3046, -0.0459, -0.1348,\n",
      "          0.0254,  0.4173],\n",
      "        [ 0.1985, -0.0448, -0.2171,  0.3067,  0.0311, -0.2138, -0.0950, -0.1290,\n",
      "          0.0707,  0.6437],\n",
      "        [ 0.2949,  0.1009, -0.4130,  0.1254, -0.0622, -0.4116, -0.0914,  0.0814,\n",
      "          0.0252,  0.3516],\n",
      "        [ 0.2482, -0.0756, -0.6072,  0.1497, -0.0643, -0.2225,  0.0069, -0.1661,\n",
      "          0.0760,  0.3967],\n",
      "        [ 0.2006,  0.2145, -0.4609,  0.1922, -0.2438, -0.1063, -0.0483,  0.0916,\n",
      "          0.1057,  0.2624],\n",
      "        [ 0.2335,  0.2195, -0.7438,  0.1894, -0.0762, -0.5368, -0.1450, -0.0568,\n",
      "         -0.1280,  0.1587],\n",
      "        [ 0.1333,  0.2866, -0.3152,  0.3320,  0.0440, -0.2573,  0.0091,  0.0409,\n",
      "          0.0635,  0.2059],\n",
      "        [ 0.1318,  0.1946, -0.4008,  0.3649, -0.1307, -0.4017,  0.0106, -0.2421,\n",
      "          0.0550,  0.5723],\n",
      "        [ 0.0947,  0.2694, -0.3208,  0.2499, -0.0761, -0.1961, -0.2949, -0.0923,\n",
      "          0.1294,  0.2392],\n",
      "        [ 0.1664,  0.1139, -0.5583,  0.1707, -0.3655, -0.2918, -0.0783, -0.0291,\n",
      "          0.0284,  0.3210],\n",
      "        [ 0.1470,  0.2157, -0.4040,  0.1022,  0.0604, -0.2434, -0.0897, -0.2382,\n",
      "         -0.0188,  0.2959],\n",
      "        [ 0.2997, -0.0068, -0.3348,  0.3574,  0.0144, -0.3297,  0.1999, -0.0388,\n",
      "         -0.2350,  0.4083],\n",
      "        [ 0.3765,  0.2514, -0.2405,  0.1963, -0.1547, -0.1161, -0.1281, -0.1358,\n",
      "          0.0360,  0.4142],\n",
      "        [ 0.3131,  0.3003, -0.4199,  0.0754, -0.0918, -0.0274,  0.0200,  0.1232,\n",
      "         -0.1120,  0.2314],\n",
      "        [ 0.1864,  0.4733, -0.4272,  0.2037, -0.3912, -0.1751, -0.2737,  0.0473,\n",
      "          0.2244,  0.3246],\n",
      "        [ 0.1560,  0.1729, -0.3234,  0.3241, -0.0177, -0.0488,  0.1067,  0.1780,\n",
      "         -0.0580,  0.6717],\n",
      "        [ 0.0812,  0.1625, -0.1084,  0.2263, -0.0044, -0.2856, -0.0095,  0.0746,\n",
      "          0.0428,  0.4907],\n",
      "        [ 0.3053,  0.3090, -0.3365,  0.2399, -0.0844, -0.0764,  0.0345,  0.0031,\n",
      "         -0.0469,  0.3315],\n",
      "        [ 0.2971,  0.3447, -0.1635,  0.2057, -0.3471, -0.0371, -0.0158, -0.1494,\n",
      "          0.2185,  0.4625],\n",
      "        [ 0.2611,  0.1991, -0.5575,  0.3606, -0.2133, -0.1667, -0.0166, -0.0276,\n",
      "          0.0250,  0.3776],\n",
      "        [ 0.2718,  0.1093, -0.5275,  0.1464,  0.0764, -0.1947,  0.0518, -0.0134,\n",
      "         -0.0445,  0.3768],\n",
      "        [ 0.4507, -0.0247, -0.2966,  0.3128, -0.1250, -0.2202,  0.0416,  0.0496,\n",
      "          0.0954,  0.5752],\n",
      "        [ 0.2011,  0.1773, -0.5572, -0.0249, -0.0337, -0.1423, -0.0815,  0.1002,\n",
      "         -0.1279,  0.3267],\n",
      "        [ 0.2295,  0.1210, -0.4929,  0.1519, -0.0669, -0.1252, -0.0630,  0.1600,\n",
      "         -0.2268,  0.4042]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[0].size(), test.forward(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 100]) tensor([[[-0.8112,  0.5459,  0.4067,  ..., -0.1415,  0.6768,  0.7035],\n",
      "         [-0.4754,  0.6991,  0.4315,  ..., -0.6095,  0.7262,  0.7983],\n",
      "         [ 0.0216,  0.6962,  0.2407,  ..., -0.4083,  0.5292,  0.8893],\n",
      "         ...,\n",
      "         [-0.5000,  0.3047,  0.2183,  ..., -0.5036,  0.6181,  0.7862],\n",
      "         [-0.5872,  0.7028,  0.0917,  ..., -0.3595,  0.6892,  0.8751],\n",
      "         [-0.5660,  0.7532,  0.4371,  ..., -0.0735,  0.5856,  0.7527]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test.forward(x)[1].size(), test.forward(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
